# 条件本质上确界研究项目

本项目用于研究条件本质下却界。

## 文件说明


**文件夹**

- `inquality-estimation-from-data`, 数据中估计出一些不等式，本质就是必要条件或者充分条件关系
- `estcondmin`，条件本质下却界的 R 包。可以直接通过如下代码安装。

```
devtools::install_github("HeyangGong/estcondmin")
```
 


## 2025/05/21 研究动机回想

我跟你说啊，我最近在琢磨那个**条件期望 $E[Y|X]$**，感觉有点意思。

你想啊，如果我们知道了 $X$ 是啥，那我们对 $Y$ 的最好猜测是啥？这个‘最好猜测’其实就是 $E[Y|X]$。而且，这个猜测会随着 $X$ 的不同而不同，所以 $E[Y|X]$ 本身其实是 $X$ 的一个函数，咱们可以写成 $h(X)$。

然后我就在想，如果我们有一堆只用 $X$ 就能算出来的函数，比如 $X^2$啊，$\sin(X)$ 啊，或者随便一个什么 $g(X)$。在所有这些只依赖于 $X$ 的函数里面，哪一个跟我们真正关心的 $Y$ 最‘像’、最‘接近’呢？

‘接近’怎么衡量？最常用的就是看它们差的平方的平均值，也就是均方误差 $E[(Y - g(X))^2]$。我们想让这个误差越小越好。

神奇的地方来了！那个能让 $E[(Y - g(X))^2]$ 达到最小的 $g(X)$，不多不少，正好就是咱们一开始说的那个 $E[Y|X]$！这就好像是把 $Y$ 投影到所有由 $X$ 构成的函数空间里，得到的那个‘影子’就是 $E[Y|X]$。

所以啊，咱们平时做回归分析，特别是用**最小二乘法 (MSE) 的回归**，比如弄个线性模型 $f(X) = \beta_0 + \beta_1 X$ 去拟合数据，我们的目标不就是最小化那个残差平方和嘛？其实，我们本质上就是在尝试用我们的模型 $f(X)$ 去**逼近或者估计那个真正的条件期望 $E[Y|X]$**！

所以说，MSE 回归的目标，从理论上讲，就是想找到那个条件期望。酷不酷？


如果我们用期望绝对误差 E[|Y - f(X)|] 来定义“接近”，那么最优的 f(X) 不再是条件期望，而是条件中位数。条件本质上确界是什么？



## 超越平均：条件分位数与条件本质确界

我们已经知道，条件期望 $E[Y|X]$ 是在给定 $X$ 的信息下，对 $Y$ 在均方误差 (MSE) 意义下的最佳预测。它告诉我们 $Y$ 在条件下的“中心趋势”或“平均表现”。然而，很多时候我们不仅关心平均情况，还可能关心分布的其他方面，比如某个特定的分位点，或者变量可能达到的“边界”。

### 1. 条件分位数 (Conditional Quantiles) - $Q_\tau[Y|X]$

**动机：**
*   **稳健性**：条件期望（均值）对异常值很敏感。如果我们想要一个对异常值不那么敏感的条件预测，条件中位数（一种条件分位数）会是更好的选择。
*   **风险评估与预测区间**：在很多应用中，我们不仅想知道 $Y$ 的期望值，还想知道一个区间，使得 $Y$ 有很大概率落在这个区间内。例如，我们可能想预测明天股票价格的 5% 分位数和 95% 分位数，来构建一个 90% 的预测区间。
*   **非对称性关注**：有时我们对高估和低估的容忍度是不同的。例如，在库存管理中，缺货的成本可能远高于库存过多的成本，这时我们就需要关注需求分布的较高分位数。

**什么是分位数？**
对于一个随机变量 $Z$ 和一个概率值 $\tau \in (0,1)$，其 $\tau$-分位数 $q_\tau$ 是这样一个值，它使得 $P(Z \le q_\tau) \ge \tau$ 并且 $P(Z < q_\tau) \le \tau$。如果 $Z$ 的累积分布函数 $F_Z$ 是连续且严格递增的，那么 $q_\tau = F_Z^{-1}(\tau)$，即 $P(Z \le q_\tau) = \tau$。

**条件分位数 $Q_\tau[Y|X]$**：
类似地，条件 $\tau$-分位数 $Q_\tau[Y|X]$ 是一个依赖于 $X$ 的函数（一个随机变量），它表示在给定 $X$ 的取值后，$Y$ 的条件分布的 $\tau$-分位数。也就是说，对于几乎所有 $X=x$，有 $P(Y \le Q_\tau[Y|X=x] | X=x) \approx \tau$。

**如何找到条件分位数？—— Pinball 损失函数**
就像条件期望最小化了均方误差 $E[(Y-g(X))^2]$，条件 $\tau$-分位数 $Q_\tau[Y|X]$ 最小化的是一个特定的期望损失，这个损失函数叫做 **Pinball Loss** (或 Quantile Loss)：

$L_\tau(y, \hat{y}) = \begin{cases} \tau (y - \hat{y}) & \text{if } y - \hat{y} \ge 0 \quad (\text{即 } y \ge \hat{y}, \text{ 高估真实值}) \\ (\tau - 1) (y - \hat{y}) & \text{if } y - \hat{y} < 0 \quad (\text{即 } y < \hat{y}, \text{ 低估真实值}) \end{cases}$

或者等价地写成：
$L_\tau(y, \hat{y}) = (y - \hat{y}) (\tau - I(y - \hat{y} < 0))$
其中 $I(\cdot)$ 是指示函数。

这个损失函数的特点是**非对称性**：
*   当 $\tau = 0.5$ (中位数) 时，$L_{0.5}(y, \hat{y}) = 0.5 |y - \hat{y}|$。最小化它的期望 $E[0.5|Y-g(X)|]$ 等价于最小化平均绝对误差 (MAE) $E[|Y-g(X)|]$，其最优解就是条件中位数 $Med[Y|X]$。
*   当 $\tau > 0.5$ 时 (例如 $\tau = 0.9$，我们关心较高的分位数)：
    *   如果 $y > \hat{y}$ (预测值低了，真实值更高)，惩罚的权重是 $\tau$ (较大)。
    *   如果 $y < \hat{y}$ (预测值高了，真实值更低)，惩罚的权重是 $| \tau - 1 | = 1-\tau$ (较小)。
    为了避免较大的惩罚，模型会倾向于给出较高的预测值 $\hat{y}$，使得 $P(Y \le \hat{y})$ 接近 $\tau$。
*   当 $\tau < 0.5$ 时 (例如 $\tau = 0.1$，我们关心较低的分位数)：
    *   如果 $y > \hat{y}$ (预测值低了)，惩罚的权重是 $\tau$ (较小)。
    *   如果 $y < \hat{y}$ (预测值高了)，惩罚的权重是 $1-\tau$ (较大)。
    为了避免较大的惩罚，模型会倾向于给出较低的预测值 $\hat{y}$，使得 $P(Y \le \hat{y})$ 接近 $\tau$。

因此，条件 $\tau$-分位数 $Q_\tau[Y|X]$ 就是那个使期望 Pinball 损失 $E[L_\tau(Y, g(X))]$ 最小的函数 $g(X)$。
$Q_\tau[Y|X] = \arg\min_{g(X)} E[L_\tau(Y, g(X))]$
（其中 $g(X)$ 是所有 $X$ 的可测函数）

### 2. 推广到边界：条件本质上确界/下确界 - `ess sup(Y|X)` 和 `ess inf(Y|X)`

条件分位数让我们能够探索条件分布的不同位置，但它们仍然是分布内部的点。如果我们想知道，在给定 $X$ 的条件下，$Y$ “几乎不可能”超过或低于哪个值呢？这时就需要条件本质上确界和下确界。

**回顾本质上确界 (Essential Supremum, `ess sup Z`)**
对于一个随机变量 $Z$，`ess sup Z` 是最小的那个数 $a$，使得 $P(Z > a) = 0$。它忽略了那些只在概率为零的集合上发生的极端值，给出了 $Z$ “本质上的”上界。类似地，`ess inf Z` 是最大的数 $b$，使得 $P(Z < b) = 0$。

**条件本质上确界 `ess sup(Y|X)`**
`ess sup(Y|X)` 是一个 $X$ 的函数（一个 $\sigma(X)$-可测的随机变量），我们记作 $S_X$。它具有以下特性：
1.  **条件上界性**：在给定 $X$ 的条件下，$Y$ 几乎必然不会超过 $S_X$。也就是说，$P(Y > S_X | X) = 0$ 几乎必然成立。这意味着对于几乎所有 $X$ 的取值 $x$，在 $X=x$ 这个条件下，$Y$ 大于 $S_X(x)$ 的概率是零。
2.  **最小性**：$S_X$ 是满足上述条件的“最小”的这样的函数。如果存在另一个 $X$ 的函数 $M_X$ 也满足 $P(Y > M_X | X) = 0$ 几乎必然，那么 $S_X \le M_X$ 几乎必然成立。

简单来说，`ess sup(Y|X)` 就是在知道了 $X$ 之后，$Y$ 能够达到的、几乎不可能被超越的那个“天花板”。

**条件本质下确界 `ess inf(Y|X)`**
类似地，`ess inf(Y|X)` 是一个 $X$ 的函数 $I_X$，满足：
1.  **条件下界性**：$P(Y < I_X | X) = 0$ 几乎必然成立。
2.  **最大性**：如果存在另一个 $X$ 的函数 $N_X$ 也满足 $P(Y < N_X | X) = 0$ 几乎必然，那么 $I_X \ge N_X$ 几乎必然成立。

简单来说，`ess inf(Y|X)` 就是在知道了 $X$ 之后，$Y$ 几乎不可能低于的那个“地板”。

**与损失函数的关系 (启发性思考，非严格定义来源)**
条件本质上确界/下确界不像条件期望和条件分位数那样直接从一个简单的期望损失函数最小化中定义出来。它们的定义更侧重于作为“最紧的几乎必然边界”。

然而，我们可以启发性地将它们与条件分位数的极限情况联系起来：
*   `ess sup(Y|X)` 在某种意义上可以被看作是当 $\tau \to 1$ 时 $Q_\tau[Y|X]$ 的极限。当 $\tau$ 非常接近 1 时 (比如 0.99999)，Pinball 损失会极度惩罚任何 $y > \hat{y}$ 的情况（高估真实值，$P(Y > \hat{y}|X)$ 应该非常小），并试图让 $\hat{y}$ 尽可能小地成为上界。
*   `ess inf(Y|X)` 在某种意义上可以被看作是当 $\tau \to 0$ 时 $Q_\tau[Y|X]$ 的极限。当 $\tau$ 非常接近 0 时 (比如 0.00001)，Pinball 损失会极度惩罚任何 $y < \hat{y}$ 的情况（低估真实值，$P(Y < \hat{y}|X)$ 应该非常小），并试图让 $\hat{y}$ 尽可能大地成为下界。

这种联系更多是直观上的。`ess sup/inf` 的正式定义是基于它们作为满足特定条件概率属性的最小上界/最大下界。

**总结比较：**

| 特性             | 条件期望 $E[Y|X]$                     | 条件 $\tau$-分位数 $Q_\tau[Y|X]$          | 条件本质上确界 `ess sup(Y|X)`                      |
| :--------------- | :------------------------------------ | :-------------------------------------- | :------------------------------------------------- |
| **核心意义**     | 条件下的均值/中心                     | 条件下分布的特定位置点                  | 条件下的“几乎必然”上界 (天花板)                      |
| **优化目标**     | 最小化 $E[(Y-g(X))^2]$ (MSE)           | 最小化 $E[L_\tau(Y,g(X))]$ (Pinball Loss) | 不是直接的简单损失最小化；是“最小的条件a.s.上界” |
| **对异常值敏感度** | 高                                    | 中 (如中位数则低)                       | 取决于条件分布的尾部（但忽略零概率事件）               |
| **用途示例**     | 平均预测                              | 风险度量 (VaR), 预测区间, 稳健回归      | 最坏/最好情况分析 (忽略零概率极端), 定义条件支撑集   |


通过理解条件分位数和条件本质上确界/下确界，我们可以从不同角度更全面地把握在给定信息 $X$ 的情况下，随机变量 $Y$ 的行为特征，而不仅仅局限于其平均表现。
